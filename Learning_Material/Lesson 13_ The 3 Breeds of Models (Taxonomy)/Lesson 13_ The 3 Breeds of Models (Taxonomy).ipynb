{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd369bcc-d19c-41ba-893b-94676cb6e1ce",
   "metadata": {},
   "source": [
    "# Module 2: Frontier Models\n",
    "## Lesson 13: The 3 Breeds of Models (Taxonomy)\n",
    "\n",
    "### üìÑ Overview\n",
    "We move beyond just \"Open vs. Closed\" and look at the internal training objective of the models. Understanding whether you are holding a **Base**, **Chat**, or **Reasoning** model dictates how you prompt it and what you use it for.\n",
    "\n",
    "### üèõÔ∏è The Hierarchy of Intelligence\n",
    "\n",
    "#### 1. Base Models (The \"Predictors\")\n",
    "* **What they are:** The raw, unruly version of the model. Trained ONLY to predict the next token in a sequence.\n",
    "* **Behavior:** If you type *\"The capital of France is\"*, it completes it with *\"Paris\"*. If you type *\"What is the capital of France?\"*, it might complete it with *\"What is the capital of Germany?\"* (because it thinks it's a list of questions).\n",
    "* **Use Case:** Rarely used directly. They are the foundation for **Fine-Tuning** (teaching a model a new language or very specific medical syntax).\n",
    "\n",
    "#### 2. Chat / Instruct Models (The \"Talkers\")\n",
    "* **What they are:** Base models that underwent **RLHF** (Reinforcement Learning from Human Feedback). They are trained on `<System>`, `<User>`, `<Assistant>` patterns.\n",
    "* **Behavior:** They understand instructions. They are polite, concise, and stay in character.\n",
    "* **Use Case:** 90% of applications. Customer support, creative writing, fast summaries.\n",
    "* **Examples:** `gpt-4o`, `llama3.2`, `claude-3.5-sonnet`.\n",
    "\n",
    "#### 3. Reasoning Models (The \"Thinkers\")\n",
    "* **What they are:** Models trained to generate a hidden **Chain of Thought (CoT)** before outputting the final answer.\n",
    "* **Behavior:** They pause to \"think\" (generating invisible tokens) to break down complex logic, math, or coding architecture.\n",
    "* **Use Case:** Coding, Math, Complex Puzzles, Architecture Design.\n",
    "* **Examples:** `o1` (OpenAI), `DeepSeek-R1`.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Key Engineering Concept: \"Budget Forcing\"\n",
    "* **The Theory:** Reasoning models have a \"Thinking Budget\" (how much time/tokens they spend pondering).\n",
    "* **The Hack:** Researchers found that you can force a model to think *deeper* and correct its own errors simply by injecting specific tokens into its thought stream.\n",
    "* **The \"Wait\" Trick:** If the model says *\"I should deploy to AWS...\"*, and you inject the word **\"Wait,\"**, the model's probability engine often predicts the next tokens as *\"Wait, let me double check cost...\"* or *\"Wait, is that secure?\"*\n",
    "* **Takeaway:** Intelligence can be artificially boosted by forcing the model to spend more compute time (tokens) on verification.\n",
    "\n",
    "### üìä Decision Matrix: Which Breed do I use?\n",
    "\n",
    "| Task | Recommended Breed | Why? |\n",
    "| :--- | :--- | :--- |\n",
    "| **Creative Writing** (Emails, Blogs) | **Chat Model** | More fluid, less robotic, faster. |\n",
    "| **Customer Support** | **Chat Model** | Fast latency, polite tone is priority. |\n",
    "| **Complex Coding** (Refactoring) | **Reasoning Model** | Needs to plan dependency trees before writing code. |\n",
    "| **Math / Logic Puzzles** | **Reasoning Model** | Chat models often hallucinate on math; Reasoning models verify steps. |\n",
    "| **Training a Custom Model** | **Base Model** | You want a blank slate without the \"Helpful Assistant\" bias. |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
