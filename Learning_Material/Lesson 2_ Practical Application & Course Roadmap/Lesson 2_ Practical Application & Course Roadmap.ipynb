{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e01131b-18bc-45e1-97b7-2e7576003d10",
   "metadata": {},
   "source": [
    "# Module 1: Course Introduction & Local Setup\n",
    "## Lesson 2: Practical Application & Course Roadmap\n",
    "\n",
    "### ðŸ“„ Overview\n",
    "In this lesson, we move beyond the basic \"Hello World\" to a practical application: using a local LLM as a **Spanish Language Tutor**. The lesson demonstrates how a generic model can be repurposed for specific tasks through conversation.\n",
    "\n",
    "Additionally, the instructor outlines the **8-Week Engineering Roadmap**:\n",
    "1.  **Foundations**: Local setup & basics.\n",
    "2.  **Frontier Models**: Working with SOTA (State of the Art) APIs.\n",
    "3.  **Open Source**: Deep dive into local model weights.\n",
    "4.  **Model Selection**: Choosing the right tool for the job.\n",
    "5.  **RAG (Retrieval Augmented Generation)**: Connecting LLMs to private data.\n",
    "6.  **Fine-Tuning (Frontier)**: Customizing closed-source models.\n",
    "7.  **Fine-Tuning (Open Source)**: Training local models on custom datasets.\n",
    "8.  **Agentic AI**: Building autonomous systems.\n",
    "\n",
    "### ðŸ—ï¸ Key Concepts\n",
    "* **Persona / System Prompting**: The technique of instructing an LLM to adopt a specific role (e.g., \"You are a Spanish Tutor\"). This restricts the model's output space to be more relevant.\n",
    "* **Chain of Thought (CoT)**: The lesson briefly highlights models that \"show their thinking\" (often in gray text) before outputting the final answer. This \"reasoning trace\" usually improves accuracy for complex tasks.\n",
    "* **Latency vs. Intelligence**: The trade-off observed when switching between small models (fast, conversational) and larger models (slower, more nuanced).\n",
    "\n",
    "### ðŸ› ï¸ Technical Implementation: The \"Spanish Tutor\"\n",
    "To turn a raw model into a specific tool (like a tutor), we must use **System Prompts**.\n",
    "*Note: In the `ollama` library, we pass this in the `messages` list with the role `'system'`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf05ac-e6bc-4984-9103-fef9b9f79f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# 1. Define the specific \"System\" instruction\n",
    "# This tells the model how to behave before the user even speaks.\n",
    "system_prompt = (\n",
    "    \"You are a helpful Spanish language tutor for a complete beginner. \"\n",
    "    \"Always respond in Spanish first, then provide the English translation in parentheses. \"\n",
    "    \"Correct any mistakes the user makes gently.\"\n",
    ")\n",
    "\n",
    "# 2. The User's input\n",
    "user_input = \"Hola. I want learn spanish. You help?\"\n",
    "\n",
    "# 3. Construct the message history\n",
    "messages = [\n",
    "    {'role': 'system', 'content': system_prompt},\n",
    "    {'role': 'user', 'content': user_input}\n",
    "]\n",
    "\n",
    "# 4. Generate the response\n",
    "print(\"Tutor is thinking...\")\n",
    "response = ollama.chat(\n",
    "    model='phi3', # or 'llama3'\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# 5. Output results\n",
    "print(\"\\n--- Interaction ---\")\n",
    "print(f\"User: {user_input}\")\n",
    "print(f\"AI: {response['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea76328-c818-477c-9dba-70bd9c2050f8",
   "metadata": {},
   "source": [
    "### ðŸ§ª Lab Notes & Engineering Log\n",
    "\n",
    "*The following experiments focus on Prompt Engineering and Observation.*\n",
    "\n",
    "#### Experiment 1: The \"System Prompt\" Impact\n",
    "**Objective:** See how much the `system` role actually changes behavior compared to just asking in the user prompt.\n",
    "**Test:**\n",
    "1.  **Without System Prompt:** User says: *\"Teach me Spanish.\"* -> Model gives a generic plan.\n",
    "2.  **With System Prompt:** (As defined in code) -> Model immediately acts as a conversation partner.\n",
    "**Observation:** Strong system prompts are essential for building \"Agents\" later in the course.\n",
    "\n",
    "#### Experiment 2: Temperature Check\n",
    "**Engineering Challenge:**\n",
    "Modify the Python code to include `options={'temperature': 0.1}` inside the `ollama.chat` call.\n",
    "* **Hypothesis:** Lower temperature (0.1) should make the \"Tutor\" more rigid and grammatical. Higher temperature (0.8) might make it more slang-heavy or creative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
