{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2448d1a-fb08-442c-8c13-84b30fc089a5",
   "metadata": {},
   "source": [
    "# üß† Engineering Deep Dive: Understanding Token Usage & Reasoning\n",
    "\n",
    "### üìÑ Overview\n",
    "As an LLM Engineer, the `CompletionUsage` object is your primary dashboard for cost and performance. It reveals not just \"how much\" text was generated, but **how the model thought** to get there. Understanding this structure is critical for optimizing latency and budget.\n",
    "\n",
    "### üîç The Anatomy of Usage\n",
    "When you receive a response from the OpenAI API (and compatible providers), it includes a `usage` dictionary.\n",
    "\n",
    "CompletionUsage(\n",
    "    prompt_tokens=14, \n",
    "    completion_tokens=652, \n",
    "    total_tokens=666, \n",
    "    completion_tokens_details=CompletionTokensDetails(\n",
    "        reasoning_tokens=576, \n",
    "        accepted_prediction_tokens=0, \n",
    "        rejected_prediction_tokens=0\n",
    "    ), \n",
    "    prompt_tokens_details=PromptTokensDetails(\n",
    "        cached_tokens=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54264ef4-6938-4dca-bf9a-4dcc56d67f54",
   "metadata": {},
   "source": [
    "üóùÔ∏è Key Metrics Explained\n",
    "1. The Basics (The Bill)\n",
    "prompt_tokens: The input you sent (System Prompt + User Message + History).\n",
    "\n",
    "completion_tokens: The total output generated by the model.\n",
    "\n",
    "total_tokens: The sum of the above. This is the number you are billed for.\n",
    "\n",
    "2. The \"Hidden\" Cost: Reasoning Tokens\n",
    "Modern \"Reasoning Models\" (like OpenAI o1 or preview models) perform a \"Chain of Thought\" (CoT) process before outputting the final answer.\n",
    "\n",
    "reasoning_tokens: Tokens generated during the model's internal scratchpad phase.\n",
    "\n",
    "Visibility: Invisible (You do not see these in the final string).\n",
    "\n",
    "Billing: Billable (You pay for them as if they were output text).\n",
    "\n",
    "The \"Visible\" Output: $$ \\text{Visible Text} = \\text{Completion Tokens} - \\text{Reasoning Tokens} $$\n",
    "\n",
    "‚ö†Ô∏è Engineering Trap: In your example, the model generated 652 tokens but 576 were reasoning. You paid for 652 tokens to get a ~76 token answer.\n",
    "\n",
    "Efficiency: ~11% of tokens were result, ~89% were overhead.\n",
    "\n",
    "Use Case: This is acceptable for complex logic, but wasteful for simple greetings.\n",
    "\n",
    "3. Prompt Caching\n",
    "cached_tokens: Tokens that the API recognized from a previous request (usually found in prompt_tokens_details).\n",
    "\n",
    "Benefit: Cached tokens are typically 50% cheaper and result in lower latency.\n",
    "\n",
    "Strategy: Structure your prompts so the static parts (huge system instructions, RAG context) come first to maximize cache hits.\n",
    "\n",
    "üõ†Ô∏è Python Utility: Usage Analyzer\n",
    "A helper function to print a clean cost/performance report after every API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe173c2d-8789-4ae4-b1fb-3134f12778c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_usage(usage_object):\n",
    "    \"\"\"\n",
    "    Parses the OpenAI Usage object to reveal hidden costs and efficiency.\n",
    "    \"\"\"\n",
    "    # 1. Basic Extraction\n",
    "    p_tokens = usage_object.prompt_tokens\n",
    "    c_tokens = usage_object.completion_tokens\n",
    "    t_tokens = usage_object.total_tokens\n",
    "    \n",
    "    # 2. Deep Dive extraction (handling safe access if attributes are missing)\n",
    "    details = getattr(usage_object, 'completion_tokens_details', None)\n",
    "    reasoning = getattr(details, 'reasoning_tokens', 0) if details else 0\n",
    "    \n",
    "    prompt_details = getattr(usage_object, 'prompt_tokens_details', None)\n",
    "    cached = getattr(prompt_details, 'cached_tokens', 0) if prompt_details else 0\n",
    "    \n",
    "    # 3. Calculations\n",
    "    visible_tokens = c_tokens - reasoning\n",
    "    reasoning_ratio = (reasoning / c_tokens) * 100 if c_tokens > 0 else 0\n",
    "    \n",
    "    # 4. The Report\n",
    "    print(f\"üìä --- Tokenomics Report ---\")\n",
    "    print(f\"Total Billable:   {t_tokens}\")\n",
    "    print(f\"Input (Prompt):   {p_tokens} (Cached: {cached})\")\n",
    "    print(f\"Output (Total):   {c_tokens}\")\n",
    "    print(f\"  ‚îú‚îÄ Visible:     {visible_tokens}\")\n",
    "    print(f\"  ‚îî‚îÄ Reasoning:   {reasoning} ({reasoning_ratio:.1f}% of output)\")\n",
    "    \n",
    "    if reasoning_ratio > 50:\n",
    "        print(\"\\n‚ö†Ô∏è NOTE: High reasoning overhead. Ensure this task requires deep logic.\")\n",
    "\n",
    "# Example Usage with your data:\n",
    "# (You would pass the actual response.usage object here)\n",
    "from types import SimpleNamespace # Just for mocking the object in this example\n",
    "\n",
    "mock_usage = SimpleNamespace(\n",
    "    prompt_tokens=14,\n",
    "    completion_tokens=652,\n",
    "    total_tokens=666,\n",
    "    completion_tokens_details=SimpleNamespace(reasoning_tokens=576),\n",
    "    prompt_tokens_details=SimpleNamespace(cached_tokens=0)\n",
    ")\n",
    "\n",
    "analyze_usage(mock_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a4383-5f9f-4de4-abc1-6da0871f2660",
   "metadata": {},
   "source": [
    "### üß™ Lab Notes: Optimization Strategy\n",
    "\n",
    "*Based on the analysis of Reasoning Tokens, I have established the following heuristic for model selection:*\n",
    "\n",
    "| Task Type | Recommended Model Class | Why? |\n",
    "| :--- | :--- | :--- |\n",
    "| **Creative Writing / Chat** | Standard (GPT-4o, Llama 3) | Low reasoning overhead; we want to pay for visible text. |\n",
    "| **Complex Math / Coding** | Reasoning (o1, o1-mini) | The hidden `reasoning_tokens` are necessary to ensure accuracy. |\n",
    "| **Simple Extraction** | Small/Fast (GPT-4o-mini) | Avoid paying for \"thinking\" when the task is trivial. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ef9ef-ff39-474d-aff4-4703ec88e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_optimized(question):\n",
    "    \"\"\"\n",
    "    Demonstrates how to strip away token usage for simple queries.\n",
    "    \"\"\"\n",
    "    \n",
    "    # üî¥ OPTION A: The \"Expensive\" Way (Default/Reasoning)\n",
    "    # Using a reasoning model or a chatty prompt\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    \n",
    "    # üü¢ OPTION B: The \"Optimized\" Way\n",
    "    # 1. Model: Use 'gpt-4o-mini' (Cheaper, faster, no hidden reasoning tokens)\n",
    "    # 2. System Prompt: Enforce extreme brevity.\n",
    "    # 3. Max_Tokens: Hard limit to prevent rambling.\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                # The \"Concise\" Instruction is the biggest token saver\n",
    "                \"content\": \"You are a precise database. Answer immediately. No filler words. No full sentences.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        max_tokens=10,  # Hard cap. If it needs more than 10 tokens for a simple fact, it's failing.\n",
    "        temperature=0   # Deterministic. Don't get creative.\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.content\n",
    "    usage = response.usage\n",
    "    \n",
    "    print(\"\\n--- ‚úÖ Optimized Result ---\")\n",
    "    print(f\"Answer: {result}\")\n",
    "    print(f\"Total Tokens Billable: {usage.total_tokens}\")\n",
    "    print(f\"  (Prompt: {usage.prompt_tokens}, Completion: {usage.completion_tokens})\")\n",
    "\n",
    "# Test it\n",
    "ask_optimized(\"What is the capital of France?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
