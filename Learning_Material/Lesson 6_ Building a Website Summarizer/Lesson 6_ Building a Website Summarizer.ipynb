{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3616864a-68f0-4fa6-961a-2650109207dd",
   "metadata": {},
   "source": [
    "# Module 1: Course Introduction & Local Setup\n",
    "## Lesson 6: Building a Website Summarizer\n",
    "\n",
    "### üìÑ Overview\n",
    "In this lesson, we combine all previous concepts‚Äîweb scraping, system prompting, and API calls‚Äîto build a functioning application. We create a reusable pipeline that takes a URL, extracts its text, and uses GPT-4o-mini to generate a summary in a specific tone (e.g., \"Snarky\" or \"Professional\").\n",
    "\n",
    "### üóùÔ∏è Key Concepts\n",
    "* **Prompt Construction Functions**: Instead of hardcoding prompts, we write Python functions (`messages_for`) to dynamically build the context window based on input data.\n",
    "* **Tone Engineering**: Modifying the `system` role to drastically change the output style without changing the underlying data.\n",
    "* **Server-Side Scraping**: The lesson uses a basic `requests` or `BeautifulSoup` approach.\n",
    "    * *Limitation:* This only works for static HTML. It fails on Single Page Applications (SPAs) like React/Vue sites that require JavaScript rendering.\n",
    "\n",
    "### üõ†Ô∏è Technical Implementation: The Pipeline\n",
    "The architecture consists of three stages:\n",
    "1.  **Fetch**: Get raw text from URL.\n",
    "2.  **Construct**: Format text into User/System messages.\n",
    "3.  **Inference**: Send to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df59dcf2-0eab-415b-82d3-0a80391d818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# --- Helper 1: The Scraper ---\n",
    "def fetch_website_contents(url):\n",
    "    \"\"\"\n",
    "    A basic scraper that fetches HTML and strips tags.\n",
    "    Note: Fails on JS-heavy sites (use Selenium/Playwright for those).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Get text and clean up whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        # TRUNCATE to avoid blowing up the context window (token limit)\n",
    "        return text[:10000] \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching {url}: {e}\"\n",
    "\n",
    "# --- Helper 2: The Prompt Builder ---\n",
    "def messages_for(website_text, tone=\"professional\"):\n",
    "    \"\"\"\n",
    "    Dynamically builds the prompt based on the desired tone.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an assistant that analyzes the contents of a website and provides a short summary. \"\n",
    "        \"Ignore navigation text or cookies/ads.\"\n",
    "    )\n",
    "    \n",
    "    if tone == \"snarky\":\n",
    "        system_prompt = (\n",
    "            \"You are a snarky, sarcastic assistant. \"\n",
    "            \"Roast the website while summarizing it. Make fun of their marketing buzzwords.\"\n",
    "        )\n",
    "    elif tone == \"pirate\":\n",
    "        system_prompt = \"You are a pirate captain. Summarize this in sea-speak.\"\n",
    "\n",
    "    user_prompt = f\"Here is the website text:\\n\\n{website_text}\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "# --- Main Function ---\n",
    "def summarize(url, tone=\"professional\"):\n",
    "    print(f\"üåç Fetching {url}...\")\n",
    "    text = fetch_website_contents(url)\n",
    "    \n",
    "    print(f\"ü§ñ Summarizing (Tone: {tone})...\")\n",
    "    messages = messages_for(text, tone)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# --- Execution ---\n",
    "url_to_test = \"https://example.com\" # Replace with a real news site\n",
    "print(summarize(url_to_test, tone=\"snarky\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f284b-575c-435f-b44c-3509887821c3",
   "metadata": {},
   "source": [
    "### üß™ Lab Notes & Engineering Log\n",
    "\n",
    "#### Experiment 1: Tone Consistency\n",
    "**Objective:** See if the \"Snarky\" persona survives long documents.\n",
    "**Test:**\n",
    "* I fed it a serious financial report.\n",
    "* **Result:** It started snarky but drifted back to serious tone by the end.\n",
    "* **Insight:** For long contexts, you often need to reiterate the persona at the *end* of the prompt as well: *\"Remember to stay snarky!\"*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
