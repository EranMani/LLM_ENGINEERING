{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb85bf12-b695-4945-9c55-fb505e0a2d07",
   "metadata": {},
   "source": [
    "# Module 1: Course Introduction & Local Setup\n",
    "## Lesson 1: Getting Started with Ollama & Local LLMs\n",
    "\n",
    "### üìÑ Overview\n",
    "This lesson establishes the foundation for the course by setting up a local development environment. We move away from relying solely on cloud APIs (like OpenAI) and learn to host Open Source Large Language Models (LLMs) on our own hardware using **Ollama**.\n",
    "\n",
    "### üóùÔ∏è Key Concepts\n",
    "* **Local Inference**: Running AI models on personal hardware (CPU/GPU).\n",
    "    * *Pros:* Privacy, zero incremental cost, no rate limits.\n",
    "    * *Cons:* Hardware dependent (RAM/VRAM), high energy usage.\n",
    "* **Ollama**: A runtime manager that simplifies downloading, installing, and running LLMs via a simple CLI or API.\n",
    "* **Parameter Count**: The primary metric for model size.\n",
    "    * **Small (e.g., Gemma 2B)**: Runs on almost any laptop; good for simple tasks.\n",
    "    * **Medium (e.g., Phi-3 3.8B)**: Good balance of speed and reasoning.\n",
    "    * **Large (e.g., Llama-3 8B+)**: Requires dedicated hardware (16GB+ RAM recommended).\n",
    "\n",
    "### üõ†Ô∏è Technical Implementation\n",
    "We interact with the local models using the `ollama` Python library. This allows us to integrate LLMs directly into our Python applications.\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Install Ollama software: [ollama.com](https://ollama.com)\n",
    "2. Install Python library: `pip install ollama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66769e95-3e9a-41b3-8c82-87956668ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# 1. Pull the model programmatically\n",
    "# (This ensures the model weights are downloaded to your machine)\n",
    "print(\"Checking/Downloading model 'phi3'...\")\n",
    "ollama.pull('phi3')\n",
    "\n",
    "# 2. Define the prompt\n",
    "user_prompt = \"Explain the concept of 'Open Source Weights' in one sentence.\"\n",
    "\n",
    "# 3. Generate a response using the chat method\n",
    "# We use 'chat' instead of 'generate' to support conversation history (roles)\n",
    "response = ollama.chat(\n",
    "    model='phi3',\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': user_prompt,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Print the result\n",
    "print(\"\\n--- Model Response ---\")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f29b57-0279-4eb1-914e-77c7faae7d0d",
   "metadata": {},
   "source": [
    "### üß™ Lab Notes & Engineering Log\n",
    "\n",
    "#### Experiment 1: Streaming Responses\n",
    "**Engineering Challenge:**\n",
    "In a real application, we don't wait for the full text. I modified the code to use `stream=True`.\n",
    "\n",
    "*Code snippet used for streaming:*\n",
    "```python\n",
    "stream = ollama.chat(model='phi3', messages=[{'role': 'user', 'content': 'Tell a story'}], stream=True)\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
